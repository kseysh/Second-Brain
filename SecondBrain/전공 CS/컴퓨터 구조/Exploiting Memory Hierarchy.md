## 지역성의 원리
• 프로그램은 언제든지 전체 주소 공간 중 일부만 접근함
• 시간 지역성
	• 최근에 접근한 항목은 곧 다시 접근될 가능성이 높음
	• 예: 반복문 안의 명령어, 유도 변수
• 공간 지역성
	• 최근에 접근한 항목 근처에 있는 항목도 곧 접근될 가능성이 높음
	• 예: 순차적인 명령어 접근, 배열 데이터
## 지역성을 활용하는 방법
• 메모리 계층 구조
	• 모든 데이터를 디스크에 저장 
	• 디스크에서 최근에 접근한(또는 근처의) 데이터를 더 작은 DRAM으로 복사
		• 주기억장치
	• DRAM에서 최근에 접근한(또는 근처의) 데이터를 더 작은 SRAM으로 복사
		• CPU에 연결된 캐시 메모리
## 메모리 계층 구조의 단계
• 블록(또는 라인): 복사의 단위
	• 하나 이상의 word를 포함할 수 있음
• 접근한 데이터가 상위 계층에 있을 경우
	• 히트(hit): 접근이 상위 계층에서 만족됨
		• 히트율(hit ratio): 히트 횟수 / 총 접근 횟수
• 접근한 데이터가 없을 경우
	• 미스(miss): 블록을 하위 계층에서 복사해 옴
		• 소요 시간: 미스 패널티(miss penalty)
		• 미스율(miss ratio): 미스 횟수 / 총 접근 횟수
			= 1 - 히트율
	• 그 후 데이터는 상위 계층에서 제공됨
![[Pasted image 20250525200305.png|200]]
## 메모리 기술
• 정적 RAM(SRAM) - Cache
	• 접근 시간: 0.5ns ~ 5ns, 가격: GB당 $400 ~ $800
• 동적 RAM(DRAM) - Main memory
	• 접근 시간: 20ns ~ 50ns, 가격: GB당 $2 ~ $4
• 낸드 플래시(SSD)
	• 접근 시간: 250μs ~ 1000μs, 가격: GB당 $0.04 ~ $0.10
• 자기 디스크
	• 접근 시간: 2ms ~ 20ms, 가격: GB당 $0.01 ~ $0.02
• 이상적인 메모리
	• SRAM의 접근 시간
	• 디스크의 용량과 GB당 비용
## 캐시 메모리
• 캐시 메모리
	• CPU에 가장 가까운 메모리 계층
• 접근 순서 X1, …, Xn–1, Xn이 주어졌을 때
	• 데이터가 있는지 어떻게 알 수 있을까?
	• 어디에서 찾아야 할까?
![[Pasted image 20250525200320.png|200]]
## Direct Mapped Cache 
(주소마다 저장될 수 있는 칸을 고정)
• 위치는 주소에 따라 결정됨
• Direct Mapped: 선택지는 하나뿐
	• (블록 주소) modulo (캐시 블록 수)
		• 블록 수는 2의 거듭제곱
		• 주소의 하위 비트를 사용
![[Pasted image 20250525200334.png|200]]
## 태그와 유효 비트
• 특정 캐시 위치에 어떤 블록이 저장되어 있는지 어떻게 알까?
	• 데이터뿐 아니라 블록 주소도 함께 저장
	• 실제로는 상위 비트만 필요
	• 이를 태그(tag)라고 함
• 해당 위치에 데이터가 없으면?
	• 유효 비트(valid bit): 1 = 존재, 0 = 없음
	• 초기값은 0
주소의 하위 bits -> 위치를 찾기
주소의 상위 bits -> 맞는지 확인
valid bit -> 유효한지 확인
## Cache Example
8-blocks, 1 word/block, direct mapped
initial state
![[Pasted image 20250527213151.png|200]]

![[Pasted image 20250527213213.png|200]]

![[Pasted image 20250527213229.png|200]]

![[Pasted image 20250527213242.png|200]]
![[Pasted image 20250527213300.png|200]]

![[Pasted image 20250527213314.png|200]]
11010이 쫒겨나는 것
=> evict
## Address Subdivision
![[Pasted image 20250527213810.png|400]]
Tag끼리 비교해보고 같으면 valid bit를 y로 한다.
Data

예시: 더 큰 블록 크기
• 64개의 블록, 블록당 16바이트
• 주소 1200은 어떤 블록 번호에 대응되는가?
• 블록 주소 = ⌊1200 ÷ 16⌋ = 75
• 블록 번호 = 75 mod 64 = 11

블록 크기 고려사항
• 더 큰 블록은 miss 비율을 줄일 수 있음
• 이는 공간 지역성(spatial locality) 덕분
• 하지만 캐시 크기가 고정되어 있다면
• 블록이 클수록 블록 수는 줄어듦
• 블록 간 경쟁이 심해져 miss 비율 증가
• 더 큰 블록은 캐시 오염(pollution) 가능성 증가
• miss penalty도 더 커짐
• 이로 인해 miss 비율 감소의 이점이 상쇄될 수 있음
• early restart와 critical-word-first가 도움이 될 수 있음

캐시 미스(Cache Misses)
• 캐시 hit 시, CPU는 정상적으로 진행
• 캐시 miss 시
• CPU 파이프라인 정지
• 다음 계층에서 블록을 가져옴
• 명령어 캐시 미스: 명령어 fetch 재시작
• 데이터 캐시 미스: 데이터 접근 완료 후 진행

Write-Through
• 데이터 쓰기 hit 시, 캐시만 업데이트할 수도 있음
• 그러나 캐시와 메모리의 불일치 발생 가능
• write-through: 메모리도 함께 업데이트
• 하지만 쓰기 시간이 길어짐
• 예: base CPI = 1, 명령의 10%가 store, 메모리 쓰기 = 100 사이클
• 유효 CPI = 1 + 0.1 × 100 = 11
• 해결책: write buffer
• 메모리에 쓰기 대기 중인 데이터를 임시 저장
• CPU는 즉시 다음 명령 수행 가능
• write buffer가 가득 찼을 때만 stall 발생

Write-Back
• 대안: 데이터 쓰기 hit 시 캐시만 업데이트
• 각 블록이 변경되었는지(dirty) 추적
• dirty 블록이 교체될 때만 메모리에 쓰기
• write buffer를 사용해 교체 전 블록 읽기를 먼저 수행 가능

Write Allocation
• 쓰기 miss 시 어떻게 처리할 것인가?
• write-through의 경우
• Allocate on miss: 블록을 가져옴
• Write around: 블록을 가져오지 않음
• 프로그램은 종종 블록 전체를 먼저 쓰기 때문에 (예: 초기화)
• write-back의 경우
• 일반적으로 블록을 가져옴

예시: Intrinsity FastMATH
• 내장형 MIPS 프로세서
• 12단계 파이프라인
• 매 사이클마다 명령어와 데이터 접근
• 분리된 캐시: I-cache, D-cache 분리
• 각각 16KB: 256 블록 × 16 단어/블록
• D-cache는 write-through 또는 write-back 지원
• SPEC2000 miss 비율
• I-cache: 0.4%
• D-cache: 11.4%
• 가중 평균: 3.2%

주 메모리와 캐시 지원
• 메인 메모리로 DRAM 사용
• 고정 폭 (예: 1단어)
• 고정 폭 클럭 버스로 연결
• 버스 클럭은 일반적으로 CPU 클럭보다 느림
• 예시 캐시 블록 읽기
• 주소 전송: 1 버스 사이클
• DRAM 접근: 단어당 15 버스 사이클
• 데이터 전송: 단어당 1 버스 사이클
• 1단어 폭 DRAM, 4단어 블록이면
• miss penalty = 1 + 4×15 + 4×1 = 65 버스 사이클
• 대역폭 = 16 바이트 / 65 사이클 = 0.25 B/cycle

캐시 성능 측정
• CPU 시간 구성 요소
• 프로그램 실행 사이클
• 캐시 hit 시간 포함
• 메모리 stall 사이클
• 주로 캐시 miss로부터 발생
• 단순화된 가정 하에서:

캐시 성능 예시
• 주어진 조건
• I-cache miss율 = 2%
• D-cache miss율 = 4%
• miss penalty = 100 사이클
• 기본 CPI (이상적 캐시) = 2
• load & store 비율 = 36%
• 명령어당 miss 사이클
• I-cache: 0.02 × 100 = 2
• D-cache: 0.36 × 0.04 × 100 = 1.44
• 실제 CPI = 2 + 2 + 1.44 = 5.44
• 이상적인 CPU보다 5.44 / 2 = 2.72배 느림

평균 접근 시간
• hit 시간도 성능에 중요
• 평균 메모리 접근 시간 (AMAT)
• AMAT = Hit time + Miss rate × Miss penalty
• 예시
• CPU 클럭 1ns, hit time = 1 사이클, miss penalty = 20 사이클, I-cache miss율 = 5%
• AMAT = 1 + 0.05 × 20 = 2ns

성능 요약
• CPU 성능이 향상되면
• miss penalty의 중요도가 커짐
• 기본 CPI 감소 → 메모리 stall이 차지하는 비중 증가
• 클럭 속도 증가 → 메모리 stall이 더 많은 CPU 사이클 소모
• 시스템 성능 평가 시 캐시 동작 무시할 수 없음

Associative Caches
• 완전 연관(fully associative)
• 특정 블록을 어떤 캐시 엔트리에도 저장 가능
• 모든 엔트리를 동시에 비교해야 함
• 엔트리마다 비교기 필요 → 비용 큼
• n-way 집합 연관(n-way set associative)
• 각 집합이 n개의 엔트리 포함
• 블록 번호가 어느 집합에 속할지 결정
• (블록 번호) mod (캐시의 집합 수)
• 특정 집합 내 모든 엔트리 동시에 검색
• 비교기 n개만 필요 → 비용 절감

다음은 해당 글의 한글 번역입니다:

⸻

연관도(Associativity) 예시
• 4블록 캐시 비교
• 직접 매핑(direct mapped), 2-way 집합 연관(set associative), 완전 연관(fully associative)
• 블록 접근 순서: 0, 8, 0, 6, 8

연관도는 얼마나 필요할까?
• 연관도가 증가하면 miss 비율은 감소함
• 하지만 점점 그 효과는 줄어듦
• 64KB D-cache, 16-word 블록, SPEC2000 기준 시뮬레이션 결과:
• 1-way: 10.3%
• 2-way: 8.6%
• 4-way: 8.3%
• 8-way: 8.1%

교체 정책(Replacement Policy)
• 직접 매핑: 선택 여지 없음 (고정 위치)
• 집합 연관:
• 유효하지 않은 엔트리가 있다면 그것을 우선 선택
• 그렇지 않으면 집합 내 엔트리 중 하나를 선택
• 최소 최근 사용(LRU): 가장 오랫동안 사용되지 않은 블록 선택
• 2-way는 간단, 4-way는 가능하지만 복잡, 그 이상은 너무 어려움
• 랜덤(Random):
• 연관도가 높을 때는 LRU와 거의 유사한 성능 제공

다단계 캐시(Multilevel Caches)
• 1차 캐시(L1): CPU에 직접 연결됨
• 작지만 매우 빠름
• 2차 캐시(L2): L1 miss를 처리
• 더 크고 느리지만 메인 메모리보다는 빠름
• 메인 메모리: L2 miss를 처리
• 일부 고급 시스템은 **3차 캐시(L3)**도 포함

다단계 캐시 예시
• 조건:
• CPU base CPI = 1, 클럭 속도 = 4GHz
• miss 비율/명령어 = 2%
• 메인 메모리 접근 시간 = 100ns
• 1차 캐시만 사용 시
• miss penalty = 100ns / 0.25ns = 400 사이클
• 유효 CPI = 1 + 0.02 × 400 = 9

다단계 캐시 예시 (계속)
• 이제 L2 캐시 추가
• 접근 시간 = 5ns
• 메인 메모리까지의 전체 miss 비율 = 0.5%
• L1 miss → L2 hit
• 패널티 = 5ns / 0.25ns = 20 사이클
• L1 miss → L2 miss
• 추가 패널티 = 400 사이클
• 실제 CPI = 1 + 0.02 × 20 + 0.005 × 400 = 3.4
• 성능 향상 비율 = 9 / 3.4 = 2.6

다단계 캐시 고려사항
• 1차 캐시 (L1)
• hit 시간을 최소화하는 데 중점
• 2차 캐시 (L2)
• miss 비율을 낮추는 데 중점 (메인 메모리 접근 방지)
• hit 시간은 상대적으로 덜 중요
• 그 결과:
• L1 캐시는 단일 캐시보다 작음
• L1 블록 크기는 L2 블록 크기보다 작음

고급 CPU와의 상호작용
• out-of-order CPU는 캐시 miss 중에도 명령 실행 가능
• 대기 중인 store는 load/store 유닛에 유지
• 의존 명령은 reservation station에서 대기
• 독립 명령은 계속 실행
• miss의 영향은 프로그램의 데이터 흐름에 따라 달라짐
• 분석이 매우 어려움 → 시스템 시뮬레이션 필요

소프트웨어와의 상호작용
• miss는 메모리 접근 패턴에 따라 달라짐
• 알고리즘 동작
• 컴파일러의 메모리 접근 최적화

Blocking을 통한 소프트웨어 최적화
• 목표: 데이터를 교체하기 전에 최대한 자주 접근
• 예: DGEMM(행렬 곱셈) 내부 루프 고려

⸻
