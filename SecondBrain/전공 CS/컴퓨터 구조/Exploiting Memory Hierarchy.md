## 지역성의 원리
• 프로그램은 언제든지 전체 주소 공간 중 일부만 접근함
• 시간 지역성
	• 최근에 접근한 항목은 곧 다시 접근될 가능성이 높음
	• 예: 반복문 안의 명령어, 유도 변수
• 공간 지역성
	• 최근에 접근한 항목 근처에 있는 항목도 곧 접근될 가능성이 높음
	• 예: 순차적인 명령어 접근, 배열 데이터
## 지역성을 활용하는 방법
• 메모리 계층 구조
	• 모든 데이터를 디스크에 저장 
	• 디스크에서 최근에 접근한(또는 근처의) 데이터를 더 작은 DRAM으로 복사
		• 주기억장치
	• DRAM에서 최근에 접근한(또는 근처의) 데이터를 더 작은 SRAM으로 복사
		• CPU에 연결된 캐시 메모리
## 메모리 계층 구조의 단계
• 블록(또는 라인): 복사의 단위
	• 하나 이상의 word를 포함할 수 있음
• 접근한 데이터가 상위 계층에 있을 경우
	• 히트(hit): 접근이 상위 계층에서 만족됨
		• 히트율(hit ratio): 히트 횟수 / 총 접근 횟수
• 접근한 데이터가 없을 경우
	• 미스(miss): 블록을 하위 계층에서 복사해 옴
		• 소요 시간: 미스 패널티(miss penalty)
		• 미스율(miss ratio): 미스 횟수 / 총 접근 횟수
			= 1 - 히트율
	• 그 후 데이터는 상위 계층에서 제공됨
![[Pasted image 20250525200305.png|200]]
## 메모리 기술
• 정적 RAM(SRAM) - Cache
	• 접근 시간: 0.5ns ~ 5ns, 가격: GB당 $400 ~ $800
• 동적 RAM(DRAM) - Main memory
	• 접근 시간: 20ns ~ 50ns, 가격: GB당 $2 ~ $4
• 낸드 플래시(SSD)
	• 접근 시간: 250μs ~ 1000μs, 가격: GB당 $0.04 ~ $0.10
• 자기 디스크
	• 접근 시간: 2ms ~ 20ms, 가격: GB당 $0.01 ~ $0.02
• 이상적인 메모리
	• SRAM의 접근 시간
	• 디스크의 용량과 GB당 비용
## 캐시 메모리
• 캐시 메모리
	• CPU에 가장 가까운 메모리 계층
• 접근 순서 X1, …, Xn–1, Xn이 주어졌을 때
	• 데이터가 있는지 어떻게 알 수 있을까?
	• 어디에서 찾아야 할까?
![[Pasted image 20250525200320.png|200]]
## Direct Mapped Cache 
(주소마다 저장될 수 있는 칸을 고정)
• 위치는 주소에 따라 결정됨
• Direct Mapped: 선택지는 하나뿐
	• (블록 주소) modulo (캐시 블록 수)
		• 블록 수는 2의 거듭제곱
		• 주소의 하위 비트를 사용
![[Pasted image 20250525200334.png|200]]
## 태그와 유효 비트
• 특정 캐시 위치에 어떤 블록이 저장되어 있는지 어떻게 알까?
	• 데이터뿐 아니라 블록 주소도 함께 저장
	• 실제로는 상위 비트만 필요
	• 이를 태그(tag)라고 함
• 해당 위치에 데이터가 없으면?
	• 유효 비트(valid bit): 1 = 존재, 0 = 없음
	• 초기값은 0
주소의 하위 bits -> 위치를 찾기
주소의 상위 bits -> 맞는지 확인
valid bit -> 유효한지 확인
## Cache Example
8-blocks, 1 word/block, direct mapped
initial state
![[Pasted image 20250527213151.png|200]]

![[Pasted image 20250527213213.png|200]]

![[Pasted image 20250527213229.png|200]]

![[Pasted image 20250527213242.png|200]]
![[Pasted image 20250527213300.png|200]]

![[Pasted image 20250527213314.png|200]]
11010이 쫒겨나는 것
=> evict
## Address Subdivision
![[Pasted image 20250527213810.png|400]]
1. CPU가 메모리 주소를 발생시키면, 그 주소는 Tag / Index / Offset으로 분할됨
2. Index bit를 이용해 캐시 메모리의 특정 슬롯에 접근함
3. 접근한 캐시 슬롯에 저장된 Tag와 주소의 Tag를 비교함
	1. Tag 일치 && Valid = 1 => Hit
	2. else, Miss
## 예시: 더 큰 블록 크기
• 64개의 블록, 블록당 16바이트
• 주소 1200은 어떤 블록 번호에 대응되는가?
• 블록 주소 = ⌊1200 ÷ 16⌋ = 75
• 블록 번호 = 75 mod 64 = 11
![[Pasted image 20250527221022.png|300]]
block 갯수: 2<sup>6</sup> blocks
한 block의 크기 2<sup>4</sup>byte
Direct mapped cache의 경우 => cache size: 2<sup>10</sup>byte
## 블록 크기 고려사항
• 더 큰 블록은 miss 비율을 줄일 수 있음
	• 이는 공간 지역성(spatial locality) 덕분
• 하지만 캐시 크기가 고정되어 있다면
	• 블록이 클수록 블록 수는 줄어듦
		• 블록 간 경쟁이 심해져 miss 비율 증가
	• 더 큰 블록은 캐시 오염(pollution) 가능성 증가 (pollution = 값을 가져왔지만 쓰지 않는 공간)
• miss penalty도 더 커짐 (miss일 때, data를 main memory로부터 가져오는데 걸리는 시간)
	• 이로 인해 miss 비율 감소의 이점이 상쇄될 수 있음
## 캐시 미스 (Cache Misses)
• 캐시 hit 시, CPU는 정상적으로 진행
• 캐시 miss 시
	• CPU 파이프라인 정지
	• 다음 계층에서 블록을 가져옴
	• 명령어 캐시 미스: 명령어 fetch 재시작
	• 데이터 캐시 미스: 데이터 접근 완료 후 진행
## Write-Through
• 데이터 쓰기 hit 시, 캐시만 업데이트할 수도 있음
	• 그러나 캐시와 메모리의 불일치 발생 가능
• write-through: 메모리도 함께 업데이트
• 하지만 쓰기 시간이 길어짐
	• 예: base CPI = 1, 명령의 10%가 store, 메모리 쓰기 = 100 사이클
		• 유효 CPI = 1 + 0.1 × 100 = 11
• 해결책: write buffer
	• 메모리에 쓰기 대기 중인 데이터를 임시 저장
	• CPU는 즉시 다음 명령 수행 가능
		• write buffer가 가득 찼을 때만 stall 발생
### write cache hit 
- write through: 캐시/메모리 둘 다 적용
- write back: 캐시에만 적용 (쫒겨날 때 메모리에 적용)
## Write-Back
• 대안: 데이터 쓰기 hit 시 캐시만 업데이트
	• 각 블록이 변경되었는지(dirty) 추적
		각 cache block마다 dirty bit를 둔다
		원본 데이터와 값이 달라졌다는 뜻
• dirty 블록이 교체될 때만 메모리에 쓰기
	• write buffer를 사용해 교체 전 블록 읽기를 먼저 수행 가능
## Write Allocation
일반적으로 write back + write allocation 사용
• 쓰기 miss 시 어떻게 처리할 것인가?
• write-through의 경우
	• Allocate on miss: 블록을 가져옴
	• Write around: 블록을 가져오지 않음 (보통 잘 안 쓰고, 부분적으로만 사용)
		• 프로그램은 종종 블록 전체를 먼저 쓰기 때문에 (예: 초기화)
• write-back의 경우 (보통 같이 쓰임)
	• 일반적으로 블록을 가져옴
### write miss
- write allocation -> miss일 때 fetch O
- write around -> miss일 때 fetch X
## 예시: Intrinsity FastMATH
• 내장형 MIPS 프로세서
	• 12단계 파이프라인
	• 매 사이클마다 명령어와 데이터 접근
• 분리된 캐시: I-cache, D-cache 분리
	• 각각 16KB: 256 블록 × 16 단어/블록
		16kb=> 2<sup>14</sup>byte
		2<sup>8</sup>block x 2<sup>6</sup>Byte/Block
	• D-cache는 write-through 또는 write-back 지원
	I-cache는 write이 일어나지 않음
• SPEC2000 miss 비율
	• I-cache: 0.4% (매 instruction마다 접근)
	• D-cache: 11.4% (load/store instruction만 접근)
	• 가중 평균: 3.2%
![[Pasted image 20250527230857.png|400]]
## 캐시 성능 측정
• CPU 시간 구성 요소
	• 프로그램 실행 사이클
		• 캐시 hit 시간 포함
	• 메모리 stall 사이클
		• 주로 캐시 miss로부터 발생
• 단순화된 가정 하에서:
![[Pasted image 20250527230929.png|300]]
## 캐시 성능 예시
• 주어진 조건
	• I-cache miss율 = 2%
	• D-cache miss율 = 4%
	• miss penalty = 100 사이클
	• 기본 CPI (이상적 캐시) = 2
	• load & store 비율 = 36%
• 명령어당 miss 사이클
	• I-cache: 0.02 × 100 = 2
	• D-cache: 0.36 × 0.04 × 100 = 1.44
• 실제 CPI = 2 + 2 + 1.44 = 5.44 (*기본 CPI를 더해줘야 한다는 사실을 잊지 말자*)
	• 이상적인 CPU보다 5.44 / 2 = 2.72배 느림
=> 계산 할 줄만 알면 될 듯
## 평균 접근 시간
• hit 시간도 성능에 중요
• 평균 메모리 접근 시간 (AMAT)
	• AMAT = Hit time + Miss rate × Miss penalty
• 예시
	• CPU 클럭 1ns, hit time = 1 사이클, miss penalty = 20 사이클, I-cache miss율 = 5%
	• AMAT = 1 + 0.05 × 20 = 2ns
=> 계산 할 줄만 알면 될 듯
## 성능 요약
• CPU 성능이 향상되면
	• miss penalty의 중요도가 커짐
• 기본 CPI 감소 → 메모리 stall이 차지하는 비중 증가
• 클럭 속도 증가 → 메모리 stall이 더 많은 CPU 사이클 소모
• 시스템 성능 평가 시 캐시 동작 무시할 수 없음
Cache miss로 인한 성능하락을 줄여야 함
- miss rate를 줄임
- miss penalty를 줄임
## Associative Caches
• Fully associative(완전 연관) - 모든 주소가 아무 block이나 사용가능
	• 특정 블록을 어떤 캐시 엔트리에도 저장 가능
	• 모든 엔트리를 동시에 비교해야 함
	• 엔트리마다 비교기 필요 → 비용 큼
• n-way 집합 연관(n-way set associative) - 후보지가 n개
	• 각 집합이 n개의 엔트리 포함
	• 특정 집합 내 모든 엔트리 동시에 검색
	• 비교기 n개만 필요 → 비용 절감
Direct mapped
index->block
N-way set associative
index->set
set = group of blocks
#### Example
![[Pasted image 20250529153439.png|400]]
### Spectrum of Associativity
![[Pasted image 20250529153506.png|400]]
## Associativity 예시
• 4블록 캐시 비교
• 직접 매핑(direct mapped), 2-way 집합 연관(set associative), 완전 연관(fully associative)
• 블록 접근 순서: 0, 8, 0, 6, 8
파란색: miss (아무것도 없어서)
빨간색: miss (conflict가 일어남)
#### Direct mapped
![[Pasted image 20250529153544.png|300]]
4 block 이니까 하위 2bit만 봄
#### 2-way set associative
![[Pasted image 20250529153636.png|300]]
4 block이니까 하위 1bit만 봄 (2-way니까)
#### Fully associative
![[Pasted image 20250529153653.png|300]]
그냥 다 넣음
## 연관도는 얼마나 필요할까?
• 연관도가 증가하면 miss 비율은 감소함
	• 하지만 점점 그 효과는 줄어듦
• 64KB D-cache, 16-word 블록, SPEC2000 기준 시뮬레이션 결과:
	• 1-way: 10.3%
	• 2-way: 8.6%
	• 4-way: 8.3%
	• 8-way: 8.1%
## Set Associative Cache Organization
1024 block
Direct mapped => index 10bit
4-way set associative => 256 set, index 8bit
![[Pasted image 20250529153814.png|300]]
그림은 보기만 하자
## 교체 정책(Replacement Policy)
• 직접 매핑: 선택 여지 없음 (고정 위치)
• set associative:
	• 유효하지 않은 엔트리가 있다면 그것을 우선 선택
	• 그렇지 않으면 집합 내 엔트리 중 하나를 선택
• 최소 최근 사용(LRU): 가장 오랫동안 사용되지 않은 블록 선택
	• 2-way는 간단, 4-way는 가능하지만 복잡, 그 이상은 너무 어려움
• 랜덤(Random):
	• 연관도가 높을 때는 LRU와 거의 유사한 성능 제공
## 다단계 캐시(Multilevel Caches)
• 1차 캐시(L1): CPU에 직접 연결됨
	• 작지만 매우 빠름
	• hit time을 줄이기 위해 사용
• 2차 캐시(L2): L1 miss를 처리
	• 더 크고 느리지만 메인 메모리보다는 빠름
	• miss penalty를 줄이기 위해 사용
• 메인 메모리: L2 miss를 처리
	• 일부 고급 시스템은 3차 캐시(L3)도 포함
#### Example
• 조건:
	• CPU base CPI(항상 cache hit인 이상적인 CPU) = 1, 클럭 속도 = 4GHz (clock cycle time = 0.25ns)
	• miss 비율/명령어 = 2%
	• 메인 메모리 접근 시간 = 100ns
• 1차 캐시만 사용 시
	• miss penalty = 100ns / 0.25ns = 400 사이클
	• 유효 CPI = 1 + 0.02 × 400 = 9
• 이제 L2 캐시 추가
	• 접근 시간 = 5ns
	• 메인 메모리까지의 전체 miss 비율 = 0.5% (한 instruction 당 평균적으로 몇 번의 main memory 접근을 필요로 하는지)
• L1 miss → L2 hit
	• 패널티 = 5ns / 0.25ns = 20 사이클
• L1 miss → L2 miss
	• 추가 패널티 = 400 사이클
• 실제 CPI = 1 + 0.02 × 20 + 0.005 × 400 = 3.4 
• 성능 향상 비율 = 9 / 3.4 = 2.6 
(전체의 2% -> L2 탐색)
(전체의 0.5% -> main memory 탐색)
L2 miss rate = 0.5/2 = 25%
L1 hit: 1cycle
L1 miss: 1 cycle + a
L2 hit: a = 20 cycle
L2 miss: a = 20 + 400 cycle
## 다단계 캐시 고려사항
• 1차 캐시 (L1)
	• hit 시간을 최소화하는 데 중점
• 2차 캐시 (L2)
	• miss 비율을 낮추는 데 중점 (메인 메모리 접근 방지)
	• hit 시간은 상대적으로 덜 중요
• 그 결과:
	• L1 캐시는 단일 캐시보다 작음
## 고급 CPU와의 상호작용
• out-of-order CPU는 캐시 miss 중에도 명령 실행 가능
	• 대기 중인 store는 load/store 유닛에 유지
	• 의존 명령은 reservation station에서 대기
• 독립 명령은 계속 실행
	• miss의 영향은 프로그램의 데이터 흐름에 따라 달라짐
	• *분석이 매우 어려움* → 시스템 시뮬레이션 필요
그냥, 평가하기 어렵다!
## 소프트웨어와의 상호작용
• miss는 메모리 접근 패턴에 따라 달라짐
• 알고리즘 동작
• 컴파일러의 메모리 접근 최적화
## Blocking을 통한 소프트웨어 최적화
• 목표: 데이터를 교체하기 전에 최대한 자주 접근
• 예: DGEMM(행렬 곱셈) 내부 루프 고려
```cpp
for (int j = 0; j < n; ++j)
{
	double cij = C[i][j];
	for(int k = 0; k < n; k++)
		cij += A[i][k] * B[k][j];
	C[i][j] = cij; 
}
```
C,A and B arrays
![[Pasted image 20250602150931.png|300]]
```cpp
#define BLOCKSIZE 32
void do_block (int n, int si, int sj, int sk, double **A, double *B, double *C){
	for (int i = si; i < si+BLOCKSIZE; ++i)
		for (int j = sj; j < sj+BLOCKSIZE; ++j){
			double cij = C[j+i*n]; /* cij = C[i][j] */
			for( int k = sk; k < sk+BLOCKSIZE; k++ )
				cij += A[k+i*n] * B[j+k*n];/* cij+=A[i][k]*B[k][j] */
			C[j+i*n] = cij; /* C[i][j] = cij */}
}

void dgemm (int n, double* A, double* B, double* C){
	for ( int sj = 0; sj < n; sj += BLOCKSIZE )
		for ( int si = 0; si < n; si += BLOCKSIZE )
			for ( int sk = 0; sk < n; sk += BLOCKSIZE )
				do_block(n, si, sj, sk, A, B, C);
}
```
## Blocked DGEMM Access Pattern
![[Pasted image 20250602151118.png|400]]
Matrix multiplication을 한 번에 처음부터 끝까지 수행하는 것이 아니라 Block 단위로 Matrix multiplication을 나눠서 수행한다.
이를 통해 matrix block이 cache보다 작아진다면, 대부분을 cache hit으로 처리할 수 있게 된다.
Blocking을 통해 cache를 고려해서 성능을 향상시킬 수 있다.
## Dependability
결함(Fault): 구성 요소의 실패
• 시스템 실패로 이어질 수도 있고, 아닐 수도 있음
![[Pasted image 20250602151655.png|150]]
서비스가 명세된 대로 제공됨 → 명세된 서비스에서 벗어남 → 복구
## 신뢰성 측정 지표 (Dependability Measures)
• 신뢰성(Reliability): Mean Time To Failure(MTTF) (failure가 발생할 때까지의 평균 시간)
• 서비스 중단 시간: Mean Time To Repair(MTTR) (repair에 걸리는 평균시간)
• Mean Time Between Failure:  MTBF = MTTF + MTTR
• 가용성(Availability) = MTTF / (MTTF + MTTR)
• 가용성 향상 방법
	• **MTTF 증가**: 결함 회피, 결함 허용, 결함 예측
	• **MTTR 감소**: 진단 및 수리 도구와 프로세스 향상
## The Hamming SEC Code
Single Error Correction
• 해밍 거리
	• 두 비트 패턴 간에서 서로 다른 비트의 개수
• 최소 거리 = 2 → 단일 비트 오류 검출 가능
	• 예: 패리티 코드
• 최소 거리 = 3 → 단일 오류 수정, 2비트 오류 검출 가능
#### example
1111과 0000은 hamming distance가 4 

오류 검출을 위해 2-bit를 사용해서 11, 00을 참, 거짓으로 사용할 때,
오류 발생으로 인해 10이 되면 단일 비트에 대해 오류를 검출할 수 있게 된다. (Single Error Detection)
하지만 만약 3-bit를 사용해서 111, 000을 참, 거짓으로 사용하면
오류 발생으로 인해 101이 되면 이는 111로  correction까지 가능하다 (Single Error Correction)

4가지 case로 분류 00/01/10/11로 분류하면 minimum distance는 1이 된다.
parity bit을 추가하면, 000/011/101/110 minimum distance는 2가 된다.
원래 자신의 bit를 한 번 더 추가하면, 00000/01101/10110/11011로 minimum distance가 3이 된다.
## SEC 인코딩 (Encoding SEC)
• 해밍 코드 계산 방법: (최소한의 parity bit으로 SEC를 하는 방법)
	• 왼쪽부터 비트 번호를 1번부터 매김
	• 2의 거듭제곱 위치의 비트는 패리티 비트 (1,2,4,8)
	• 각 패리티 비트는 특정 데이터 비트를 검사함
![[Pasted image 20250603140208.png|300]]
p1은 LSB가 1인 자리들의 parity
p2는 LSB에서 두 번째 자리가 1인 자리들의 parity
![[Pasted image 20250603140427.png|100]]
#### example
원본 데이터: 10011010 를 Hamming Code로 변환
![[Pasted image 20250603140540.png|200]]
=> minimum distance가 3이 된다.
## SEC 디코딩 (Decoding SEC)
• 패리티 비트의 값은 어떤 비트에 오류가 있는지를 알려줌
	• 인코딩 시 사용한 비트 번호 매김 그대로 사용
	• 예:
		• 패리티 비트 = 0000 → 오류 없음
		• 패리티 비트 = 1010 → 10번 비트가 뒤바뀜
## SEC/DED 코드
• 전체 워드에 대한 추가 패리티 비트(pn) 추가
• 해밍 거리 = 4로 만듦
• 디코딩:
	• H = SEC 패리티 비트
		• H 짝수, pn 짝수 → 오류 없음
		• H 홀수, pn 홀수 → 수정 가능한 단일 비트 오류
		• H 짝수, pn 홀수 → pn 비트에 오류
		• H 홀수, pn 짝수 → 이중 오류 발생
• 참고: ECC DRAM은 SEC/DED를 사용하여 64비트당 8비트 보호

SEC를 만들기 위해서는 M-bit data와 K-bit parity가 있을 때,
(M+K) ≤ 2<sup>K</sup>-1이어야 한다. (ex) M=64면 K는 최소 7이어야 한다)
## 가상 메모리 (Virtual Memory)
• 주 메모리를 보조 저장장치(디스크)의 “캐시”로 사용
	• CPU 하드웨어와 운영체제가 공동으로 관리
• 프로그램은 주 메모리를 공유함
	• 각 프로그램은 자주 쓰는 코드와 데이터를 담는 독립적인 가상 주소 공간을 가짐
	• 다른 프로그램으로부터 보호됨
• CPU와 OS는 가상 주소를 물리 주소로 변환
	• 가상 메모리의 “블록”은 페이지(page)
	• 가상 주소 변환 실패는 페이지 폴트(page fault)
## 주소 변환 (Address Translation)
• 고정 크기 페이지 사용 (예: 4KB)
![[Pasted image 20250602151924.png|300]]
## 페이지 폴트 페널티 (Page Fault Penalty)
• 페이지 폴트 발생 시, 디스크에서 페이지를 불러와야 함
	• 수백만 클럭 사이클 소요
	• OS 코드가 처리
• 페이지 폴트율을 최소화하려 노력
	• 완전 연결(fully associative) 배치
	• 스마트한 교체 알고리즘 사용
## 페이지 테이블 (Page Tables)
• 배치 정보 저장
	• 페이지 테이블 항목(PTE) 배열, 가상 페이지 번호로 인덱싱
	• CPU 내 페이지 테이블 레지스터는 물리 메모리 내 페이지 테이블을 가리킴
• 페이지가 메모리에 있는 경우:
	• PTE에 물리 페이지 번호 저장
	• 상태 비트(참조됨, 수정됨 등) 포함
• 페이지가 없으면:
	• PTE가 디스크 스왑 공간의 위치를 참조함
## Translation Using a Page Table
![[Pasted image 20250602152016.png|300]]
## Mapping Pages to Storage
![[Pasted image 20250602152053.png|300]]
## 교체 및 쓰기 (Replacement and Writes)
• 페이지 폴트를 줄이기 위해 LRU(가장 오래되지 않은 페이지) 교체 우선
	• PTE의 참조 비트(Reference bit, use bit)는 페이지 접근 시 1로 설정됨
	• OS가 주기적으로 0으로 초기화
	• 참조 비트가 0인 페이지는 최근에 사용되지 않은 것
• 디스크 쓰기는 수백만 사이클 소요
	• 개별 위치가 아닌 블록 단위로 작성
	• Write-through는 비현실적
	• Write-back 방식 사용
	• 페이지가 쓰이면 PTE의 dirty 비트를 설정
## TLB를 이용한 빠른 변환 (Fast Translation Using a TLB)
• 주소 변환에는 일반적으로 추가 메모리 참조가 필요
	• PTE 접근
	• 실제 메모리 접근
• 하지만 페이지 테이블 접근은 지역성이 좋음
	• 따라서 CPU 내에 PTE를 위한 고속 캐시 사용
	• 이를 TLB (Translation Look-aside Buffer) 라고 함
	• 일반적으로 16512개의 PTE 저장
		• 히트 시 0.51 사이클
		• 미스 시 10~100 사이클
		• 미스율 0.01%~1%
	• 미스 처리는 하드웨어 또는 소프트웨어가 수행
## Fast Translation Using a TLB
![[Pasted image 20250602152158.png|300]]
## TLB 미스 (TLB Misses)
• 페이지가 메모리에 있다면:
	• 메모리에서 PTE를 불러오고 다시 시도
	• 하드웨어가 처리할 수 있음
		• 복잡한 페이지 테이블 구조에서는 복잡해질 수 있음
	• 또는 소프트웨어가 처리:
		• 특수 예외 발생, 최적화된 핸들러 실행
• 페이지가 메모리에 없다면 (페이지 폴트):
	• OS가 페이지를 디스크에서 불러오고 페이지 테이블 업데이트
	• 오류를 발생시킨 명령어 다시 실행
## TLB 미스 핸들러 (TLB Miss Handler)
• TLB 미스는 다음 중 하나 의미:
	• 페이지는 있지만 TLB에 PTE 없음
	• 페이지 자체가 없음
• 대상 레지스터가 덮어쓰기 전에 TLB 미스를 인식해야 함
	• 예외 발생
• 핸들러가 메모리에서 PTE를 TLB로 복사
	• 명령어 재시작
	• 페이지가 없으면 페이지 폴트 발생
## 페이지 폴트 핸들러 (Page Fault Handler)
• 오류 발생 가상 주소를 사용하여 PTE 찾음
• 디스크에서 페이지 위치 확인
• 교체할 페이지 선택
	• dirty면 디스크에 먼저 기록
• 페이지를 메모리에 읽고 페이지 테이블 갱신
• 프로세스를 다시 실행 가능 상태로 전환
	• 오류 명령어에서 재시작
## TLB와 캐시의 상호작용 (TLB and Cache Interaction)
• 캐시 태그가 물리 주소를 사용하면
	• 캐시 조회 전에 주소 변환 필요
• 대안: 가상 주소 태그 사용
	• 그러나 aliasing 문제 발생 가능
		• 동일한 물리 주소에 대해 서로 다른 가상 주소 존재 가능
![[Pasted image 20250602152407.png|300]]
## 메모리 보호 (Memory Protection)
• 서로 다른 작업이 가상 주소 공간의 일부를 공유할 수 있음
	• 하지만 잘못된 접근으로부터 보호 필요
	• OS의 도움이 필요함
• 하드웨어의 OS 보호 지원 필요
	• 특권 슈퍼바이저 모드 (커널 모드)
	• 특권 명령
	• 페이지 테이블 및 기타 상태 정보는 슈퍼바이저 모드에서만 접근 가능
	• 시스템 호출 예외 (예: MIPS에서 syscall)
## 메모리 계층 구조
• 공통된 원칙이 메모리 계층의 모든 수준에 적용됨
	• 캐싱 개념을 기반으로 함
• 계층 구조의 각 수준에서는 다음이 필요함:
	• 블록 배치
	• 블록 탐색
	• 미스 시 교체
	• 쓰기 정책
## 블록 배치 (Block Placement)
• 연관도(associativity)에 의해 결정됨
	• 직접 사상(1-way associative):
		• 배치 위치는 하나로 고정
	• n-웨이 집합 사상(n-way set associative):
		• 하나의 집합 내에서 n개의 선택지
	• 완전 연관(fully associative):
		• 어느 위치든 가능
• 연관도가 높을수록 미스율은 낮아짐
	• 하지만 복잡성, 비용, 접근 시간이 증가함
## Finding a Block
![[Pasted image 20250602152533.png|300]]
• 하드웨어 캐시:
	• 비교 연산을 줄여 비용 절감
• 가상 메모리:
	• 전체 테이블 조회 가능하므로 완전 연관 방식이 실현 가능
	• 미스율 감소 효과 있음
## 교체 (Replacement)
• 미스가 발생했을 때 교체할 항목 선택
	• 가장 오래전에 사용된 항목 (LRU)
		• 높은 연관도에서는 하드웨어 구현이 복잡하고 비용이 큼
	• 무작위(Random)
		• LRU와 유사한 성능, 구현이 쉬움
• 가상 메모리
	• 하드웨어 지원을 통한 LRU 근사 방식 사용
## 쓰기 정책 (Write Policy)
• Write-through
	• 상위 계층과 하위 계층 모두 업데이트
	• 교체는 간단하지만, 쓰기 버퍼가 필요할 수 있음
• Write-back
	• 상위 계층만 업데이트
	• 블록이 교체될 때 하위 계층에 기록
	• 더 많은 상태 정보 유지 필요
• 가상 메모리
	• 디스크 쓰기 지연 때문에 write-back만 현실적임
## 미스의 원인 (Sources of Misses)
• 강제 미스(Compulsory miss / cold start miss)
	• 해당 블록에 처음 접근할 때 발생
• 용량 미스(Capacity miss)
	• 캐시 크기가 제한되어 발생
	• 교체된 블록에 다시 접근
• 충돌 미스(Conflict miss / collision miss)
	• 완전 연관이 아닌 캐시에서 발생
	• 집합 내 항목 간 충돌
	• 동일한 크기의 완전 연관 캐시에서는 발생하지 않음
## Cache Design Trade-offs

| **설계 변경 사항** | **미스율에 대한 영향** | **성능 저하 요인**                                   |
| ------------ | -------------- | ---------------------------------------------- |
| 캐시 크기 증가     | 용량 미스 감소       | 접근 시간이 증가할 수 있음                                |
| 연관도 증가       | 충돌 미스 감소       | 접근 시간이 증가할 수 있음                                |
| 블록 크기 증가     | 강제 미스 감소       | 미스 패널티 증가. 블록 크기가 매우 크면, 오염으로 인해 미스율이 증가할 수 있음 |
## 캐시 일관성 문제 (Cache Coherence Problem)
• 두 CPU 코어가 하나의 물리 주소 공간을 공유하는 경우
	• Write-through 캐시 사용 시 문제 발생
![[Pasted image 20250602152802.png|300]]
## 일관성의 정의 (Coherence Defined)
• 비공식적으로: 읽기 시 가장 최근의 쓰기 값을 반환
• 공식적으로:
	• 프로세서 P가 X에 쓰고 X를 다시 읽음 (중간에 다른 쓰기 없음)
	→ 읽기는 쓴 값을 반환
	• P1이 X에 쓰고, P2가 나중에 X를 읽음
	→ 읽기는 쓴 값을 반환
		• 예시의 3단계 이후 CPU B가 X를 읽는 경우 참조
	• P1이 X에 쓰고, P2도 X에 씀
	→ 모든 프로세서는 쓰기 순서를 동일하게 봐야 함
		→ 결국 X의 최종 값이 동일해야 함
## 캐시 일관성 프로토콜 (Cache Coherence Protocols)
• 다중 프로세서 환경에서 캐시가 일관성을 보장하기 위한 동작
	• 데이터 마이그레이션
		• 데이터를 로컬 캐시로 옮겨 공유 메모리 대역폭 감소
	• 읽기-공유 데이터 복제
		• 접근 경합 감소
• 스누핑 프로토콜(Snooping protocols)
	• 각 캐시가 버스의 읽기/쓰기 감시
• 디렉토리 기반 프로토콜
	• 디렉토리에 블록의 공유 상태를 기록
## Invalidating Snooping Protocols
• 캐시가 블록을 쓰기 위해 단독 접근 권한 획득
	• 버스를 통해 무효화 메시지 브로드캐스트
	• 이후 다른 캐시에서 읽으면 미스 발생
		• 데이터 소유 캐시가 업데이트된 값을 제공
![[Pasted image 20250602153002.png|300]]
## 메모리 일관성 (Memory Consistency)
• 다른 프로세서가 쓰기를 언제 인식하는가
	• “인식”이란 읽기 시 해당 값을 반환한다는 의미
	• 즉시 반영은 불가능
• 가정:
	• 쓰기가 완료되려면 모든 프로세서가 이를 인식해야 함
	• 프로세서는 쓰기와 다른 접근 순서를 변경하지 않음
• 결과:
	• P가 X를 쓰고 그다음 Y를 쓰면,
	Y를 본 프로세서는 X도 봐야 함
	• 프로세서는 읽기의 순서를 바꿀 수는 있지만, 쓰기는 불가
## Multilevel On-chip Caches
![[Pasted image 20250602153043.png|200]]
## 2-Level TLB Organization
![[Pasted image 20250602153106.png|300]]
## 멀티이슈 지원 (Supporting Multiple Issue)
• 둘 다 다중 뱅크 캐시를 사용하여 뱅크 충돌이 없다면 사이클당 여러 번 접근 가능
• Core i7 캐시 최적화:
	• 요청한 워드를 먼저 반환
	• Non-blocking 캐시
		• miss 중에도 hit 가능 (hit under miss)
		• miss 중 또 다른 miss 가능 (miss under miss)
• 데이터 프리패칭 (data prefetching)
## 주의할 점 (Pitfalls)
• 바이트 vs. 워드 주소 지정
	• 예: 32바이트 직접 사상 캐시, 4바이트 블록
		• 바이트 36 → 블록 1
		• 워드 36 → 블록 4
• 메모리 시스템 효과를 무시하고 코드 작성/생성 시
	• 예: 배열의 행 vs. 열 순회
	• 큰 stride는 지역성(locality) 저하
• 공유 L2 또는 L3 캐시를 사용하는 다중 프로세서 환경에서
	• 코어 수보다 낮은 연관도 → 충돌 미스
	• 코어 수 증가 → 연관도도 증가 필요
• AMAT로 out-of-order 프로세서의 성능 평가 시
	• 비차단 접근의 영향을 무시함
	• 시뮬레이션을 통해 평가하는 것이 바람직
• 세그먼트를 이용한 주소 범위 확장
	• 예: Intel 80286
	• 하지만 세그먼트 크기가 충분하지 않을 수 있음
	• 주소 계산 복잡해짐
• 가상 머신을 지원하지 않는 ISA에서 VMM 구현 시
	• 예: 비특권 명령어가 하드웨어 자원 접근
	• ISA 확장 필요하거나, 게스트 OS가 해당 명령어 사용 금지 필요
## 결론 (Concluding Remarks)
• 빠른 메모리는 작고, 큰 메모리는 느림
	• 우리는 빠르고 큰 메모리를 원함 
	• 캐시는 그런 환상을 제공함 
• 지역성의 원리:
	• 프로그램은 메모리 공간의 일부만 자주 사용
• 메모리 계층 구조:
	• L1 캐시 ↔ L2 캐시 ↔ … ↔ DRAM ↔ 디스크
• 메모리 시스템 설계는 다중 프로세서에서 핵심적임